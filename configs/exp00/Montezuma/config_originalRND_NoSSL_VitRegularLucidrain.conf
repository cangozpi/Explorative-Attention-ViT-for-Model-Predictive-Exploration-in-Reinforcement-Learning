[DEFAULT]
SSL_pretraining = False
freeze_shared_backbone = False

TrainMethod = original_RND
representationLearningMethod = None


EnvType = atari
EnvID = MontezumaRevengeNoFrameskip-v4

#------
Epoch = 4
MiniBatch = 16
LearningRate = 0.0001

# Shared PPO's backbone (i.e feature extractor) ->
extracted_feature_embedding_dim  = 448

# ViT backbone (0 for LUCIDRAINS_ViT, 1 for HG_ViT) -->
ViT_implementation_type = 0

# Lucidrains ViT PPO backbone -->
ViTlucidrains_use_explorativeAttn = False
ViTlucidrains_dim = 256
ViTlucidrains_patch_size = 6
ViTlucidrains_num_classes = -1
ViTlucidrains_depth = 3
ViTlucidrains_heads = 8
ViTlucidrains_mlp_dim = 1024
ViTlucidrains_dropout = 0.1
ViTlucidrains_emb_dropout = 0.1
ViTlucidrains_dim_head = 32

# HuggingFace ViT PPO backbone -->
ViTHG_use_explorativeAttn = False
ViTHG_hidden_size = 1024
ViTHG_num_hidden_layers = 12
ViTHG_num_attention_heads = 16
ViTHG_intermediate_size = 3072
ViTHG_hidden_dropout_prob = 0.0
ViTHG_attention_probs_dropout_prob = 0.0
ViTHG_initializer_range = 0.02
ViTHG_layer_norm_eps = 1e-12
ViTHG_patch_size = 12
ViTHG_qkv_bias = True
ViTHG_encoder_stride = 16


# PPO ->
PPOEps = 0.1
Entropy = 0.001

# ------ Exploration
# RND ->
NumStep = 128
MaxStepPerEpisode = 4500
LifeDone = False
StateStackSize = 4
StickyAction = True
ActionProb = 0.25
IntGamma = 0.99
Gamma = 0.999
ExtCoef = 2.
IntCoef = 1.
UpdateProportion = 0.5
UseGAE = True
GAELambda = 0.95
PreProcHeight = 84
ProProcWidth = 84
ObsNormStep = 50
UseNoisyNet = False

# CNN Actor-Critic dims (from RND): refer to https://github.com/openai/random-network-distillation/blob/master/policies/cnn_policy_param_matched.py


# ------ Representation Learning

apply_same_transform_to_batch = False

# BYOL->
BYOL_projectionHiddenSize = 4096
BYOL_projectionSize = 256
BYOL_movingAverageDecay = 0.99
BYOL_representationLossCoef = 0.01

# Barlow-Twins ->
BarlowTwinsLambda = 0.0051
BarlowTwinsProjectionSizes = [8192, 8192, 8192]
BarlowTwins_representationLossCoef = 0.01

# ------



loadModel = False
render = False
saveCkptEvery = 100
verbose_logging = False
StableEps = 1e-8
UseGPU = True
UseGradClipping = False
MaxGradNorm = 0.5


[OPTIONS]
EnvType = [atari, mario, classic_control]




# ---------------------------------------------- Reference Values:
# [DEFAULT]
# TrainMethod = RND
# representationLearningMethod = BYOL

## EnvType = mario
## EnvID = SuperMarioBros-v0
## MaxStepPerEpisode = 18000
## ExtCoef = 5.

# EnvType = atari
# EnvID = MontezumaRevengeNoFrameskip-v4

# ------
# Epoch = 4 # number of optimization epochs
# MiniBatch = 4 # number of minibatches
# LearningRate = 0.0001


# # PPO ->
# PPOEps = 0.1 # PPO clip is calculated as surr2 = clamp(ratio, 1 - PPOEps, 1 + PPOEps)
# Entropy = 0.001 # entropy coefficient

# # RND ->
# NumStep = 128
# MaxStepPerEpisode = 18000
# LifeDone = False
# StateStackSize = 4
# StickyAction = True
# ActionProb = 0.25 # sticky action probability
# IntGamma = 0.99 # gamma used for calculating the Return for intrinsic rewards (i.e. R_i = sum_over_t((intrinsic_gamma ** t) * intrinsic_reward_t)) (i.e. future reward discount factor)
# Gamma = 0.999 # gamma used for calculating the Return for extrinsic rewards (i.e. R_e = sum_over_t((intrinsic_gamma ** t) * extrinsic_reward_t) (i.e. future reward discount factor)
# ExtCoef = 2 # coefficient of extrinsic reward in the calculation of Combined Advantage (i.e. A = (A_i * IntCoef) + (A_e * ExtCoef)
# IntCoef = 1 # coefficient of intrinsic reward in the calculation of Combined Advantage (i.e. A = (A_i * IntCoef) + (A_e * ExtCoef)
# UpdateProportion = 0.25 # proportion of experience used for training predictor
# UseGAE = True
# GAELambda = 0.95 ; lambda iN GAE
# PreProcHeight = 84 # Height of image after preprocessing the state
# ProProcWidth = 84 # Width of image after preprocessing the state
# ObsNormStep = 50 # (numStep * ObsNormStep) number of initial steps are taken for initializing observation normalization
# UseNoisyNet = False

# # CNN Actor-Critic dims (from RND): refer to https://github.com/openai/random-network-distillation/blob/master/policies/cnn_policy_param_matched.py


# ------ Representation Learning

# apply_same_transform_to_batch = True # if False, then a new transformation (used for augmenting stacked states) is sampled per each element in the batch, otherwise (True) only one transformation is sampled per batch.

# # BYOL->
# BYOL_projectionHiddenSize = 896 # original on ImageNet is 4096
# BYOL_projectionSize = 256 # original on ImageNet is 256
# BYOL_movingAverageDecay = 0.99 # original on ImageNet is dynamically changing
# BYOL_representationLossCoef = 1.0 # BYOL loss is multiplied with this coefficient

# # Barlow-Twins ->
# BarlowTwinsLambda = 0.0051 # trade-off parameter lambda of the loss function
# BarlowTwinsProjectionSizes = [1024, 1024, 1024] # original on ImageNet is [8192, 8192, 8192]
# BarlowTwins_representationLossCoef = 1.0 # BarlowTwins loss is multiplied with this coefficient

# # ------



# loadModel = False
# render = False
# saveCkptEvery = 100 # after every this many episodes during training a checkpoint is saved
# StableEps = 1e-8
# # UseGPU = True
# UseGPU = False
# UseNorm = False
# ClipGradNorm = 0.5


# [OPTIONS]
# EnvType = [atari, mario]
